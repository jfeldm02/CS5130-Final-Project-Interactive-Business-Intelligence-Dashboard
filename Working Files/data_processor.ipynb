{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Upload and Validation \n",
    "- Upload CSV or Excel files through Gradio interface\n",
    "- Display basic dataset information (shape, columns, data types)\n",
    "- Show data preview (first/last N rows)\n",
    "- Handle common data issues gracefully with informative error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Data loading\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import sqlite3\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = Path('insert file path here')\n",
    "\n",
    "def summarize_directory(DIRECTORY):\n",
    "    \"\"\"\n",
    "    Describes the files input to the program. Details which files\n",
    "    are and are not able to be processed by the program. \n",
    "\n",
    "    Parameters: \n",
    "        - DIRECTORY:    Global path of file/files\n",
    "\n",
    "    Returns:\n",
    "        - Print statement\n",
    "    \"\"\"\n",
    "    print(f\"Data directory: {DIRECTORY.absolute()}\")\n",
    "    print(f\"\\nFiles found:\")\n",
    "\n",
    "    supported_files = [\n",
    "        \".csv\", \".tsv\", \".xlsx\", \n",
    "        \".xls\", \".json\", \".h5\",\n",
    "        \".hdf5\", \".parquet\", \".feather\"\n",
    "    ]\n",
    "\n",
    "    for file in DIRECTORY.glob('*'):\n",
    "        if file.is_file():\n",
    "            if file.name.endswith in supported_files:\n",
    "                print(f\"  - {file.name}\")\n",
    "            else:\n",
    "                print(f\"  - {file.name}     # WARNING: Unsupported file type! Will not be loaded.\")\n",
    "\n",
    "# Add check prompting user to continue or not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading all types of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(DIRECTORY):\n",
    "    \"\"\"\n",
    "    Uploads the files provided, handling unsupported file types, \n",
    "    and giving summaries as to which files were successfully or \n",
    "    unsuccessfully loaded.\n",
    "\n",
    "    Parameters: \n",
    "        - DIRECTORY:    Global path of file/files\n",
    "\n",
    "    Returns:\n",
    "        - data_dict:    Successfully loaded data labeled by file stem    \n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    bad_data_dict = []\n",
    "    \n",
    "    for file in DIRECTORY.glob('*'):\n",
    "        if file.is_file():\n",
    "            print(f\"Loading {file.name}...\")\n",
    "            suffix = file.suffix\n",
    "            filename = file.stem\n",
    "            \n",
    "            try:\n",
    "                if suffix == '.csv':\n",
    "                    df = pd.read_csv(file)\n",
    "                    \n",
    "                elif suffix == '.tsv':\n",
    "                    df = pd.read_csv(file, sep='\\t')\n",
    "                \n",
    "                elif suffix in ['.xlsx', '.xls']:\n",
    "                    df = pd.read_excel(file)\n",
    "                \n",
    "                elif suffix == '.json':\n",
    "                    df = pd.read_json(file)\n",
    "                \n",
    "                elif suffix == '.parquet':\n",
    "                    df = pd.read_parquet(file)\n",
    "                \n",
    "                elif suffix == '.feather':\n",
    "                    df = pd.read_feather(file)\n",
    "                \n",
    "                elif suffix in ['.h5', '.hdf5']:\n",
    "                    df = pd.read_hdf(file)\n",
    "                \n",
    "                else:\n",
    "                    print(f\"  Skipping {file.name}. Unsupported file type.\")\n",
    "                    bad_data_dict.append(filename)\n",
    "                    continue\n",
    "                \n",
    "                data_dict[filename] = df\n",
    "                print(f\"{filename} successfully loaded!\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error: Unable to load {filename}\")\n",
    "                bad_data_dict.append(filename)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"=\"*50)\n",
    "        print(\"\\nSuccesfully Loaded:\")\n",
    "        for file in data_dict:\n",
    "            print(f\"     - {file}\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        print(\"\\nFailed to Load:\")\n",
    "        for file in bad_data_dict:\n",
    "            print(f\"     - {file}\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preview, do I want to call this once and have Gradio implementation to view different files or run a loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_head_tail(data_dict, file, number_rows= 5):\n",
    "    \"\"\"\n",
    "    Shows the first and last desired rows of desired dataframe. \n",
    "\n",
    "    Parameters: \n",
    "        - data_dict:    Dictionary containing dataframe representations\n",
    "                        of initial files\n",
    "        \n",
    "        - file:         Desired dataframe to visualize\n",
    "\n",
    "    Returns:\n",
    "        - Print statement of first desired rows of dataframe    \n",
    "    \"\"\"\n",
    "\n",
    "    df = data_dict[file]\n",
    "\n",
    "    if 2*number_rows < df.shape[0]:\n",
    "        top_rows = df.head(number_rows)\n",
    "        bottom_rows = df.tail(number_rows)\n",
    "        print(f\"\\nTop {number_rows} of {file}:\")\n",
    "        print(top_rows)\n",
    "        print(\"...\")\n",
    "        print(bottom_rows)\n",
    "    else:\n",
    "        print(f\"\\nOnly {df.shape[0]} rows in {file}:\")\n",
    "        print(df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration & Summary Statistics \n",
    "- Automated data profiling\n",
    "    - Numerical columns: mean, median, std, min, max, quartiles\n",
    "    - Categorical columns: unique values, value counts, mode\n",
    "    - Missing value report \n",
    "- Correlation matrix for numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use this again later but only for this function so I made a standalone \n",
    "def null_per_column(df):\n",
    "    # Null per column\n",
    "    columns = df.columns\n",
    "    print(\"Null values per column:\")\n",
    "    for column in columns:\n",
    "        print(f'    - {column}:     {df[column].isnull().sum()}')\n",
    "\n",
    "def dataframe_profiling(data_dict, file):\n",
    "    \"\"\"\n",
    "    Profiles the desired dataframe with its info, basic statistics, and \n",
    "    null value count.  \n",
    "\n",
    "    Parameters: \n",
    "        - data_dict:    Dictionary containing dataframe representations\n",
    "                        of initial files\n",
    "        \n",
    "        - file:         Desired dataframe to visualize\n",
    "\n",
    "    Returns:\n",
    "        - Print statement of info\n",
    "        - Print statement of statistics\n",
    "        - Print statement of null values per column \n",
    "        - Print statement of duplicate rows\n",
    "    \"\"\"\n",
    "    df = data_dict[file]\n",
    "\n",
    "    # Info\n",
    "    print(df.info())\n",
    "\n",
    "    # Null per column\n",
    "    null_per_column(df)\n",
    "\n",
    "    # Duplicate rows \n",
    "    print(f\"Games_df # of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "    # Statistics\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning. Let's prompt the user with the following:\n",
    "- Cleaning your data is highly recommended. Do you want to clean the data, handling column datatypes, null values, and duplicate rows? Yes/No\n",
    "    If no -> Next\n",
    "    If yes:\n",
    "    - It is highly recommended that all columns have the same datatype, would you like to check and handle column datatypes? Yes/No\n",
    "    Add warning: This will change all untenable entries to null values. If data is poorly constructed, the analysis will be compromised. \n",
    "        If no -> Next\n",
    "        If yes:\n",
    "        - Drag and drop the columns into their respective bin. Int64, float64, bool, object, datetime64, timedelta64, category \n",
    "    - Each column now has XX null values or XX% of the total rows. Would you like to remove any columns?\n",
    "    - Regarding null values\n",
    "        - Would you like to remove rows with null values? Yes/No\n",
    "            If yes -> Next\n",
    "            If no:\n",
    "            - For each dtype column, how would you like to handle null values? \n",
    "                Offer mean, median, mode, random in range min/max for numerical\n",
    "                Offer mode or random for everything else\n",
    "    - Regarding duplicate values\n",
    "        - Would you like to remove duplicate rows in the dataset?\n",
    "            If yes -> Next\n",
    "            If no -> Next\n",
    "\n",
    "Provide data cleaning summary of choices, compare old dataset profiling to new. Prompt to accept changes or restart process.\n",
    "Add cleaned dataframe to new section for EDA.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I picture sorting the available dataframes into their respective dtype options then passing the array of columns in each datatype option into this function \n",
    "def handle_column_dtype(df, dtype_columns, dtype):\n",
    "    for column in dtype_columns:\n",
    "        df[column].astype(dtype)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Updated null per column\n",
    "null_per_column('Insert df')\n",
    "\n",
    "# I picture sorting columns into drop or not drop bins.\n",
    "def drop_columns(df, drop_columns):\n",
    "    df = df.drop(drop_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Don't need a function for drop na\n",
    "# df.dropna()\n",
    "\n",
    "# Handle nulls: Actions include, mean, median, mode, random\n",
    "def handle_nulls(df, action_columns, action):\n",
    "    \n",
    "    for column in action_columns:\n",
    "        if action == 'Mean':\n",
    "            df[column] = df[column].fillna(df[column].mean())\n",
    "        elif action == 'Median':\n",
    "            df[column] = df[column].fillna(df[column].median())\n",
    "        elif action == 'Mode':\n",
    "            df[column] = df[column].fillna(df[column].mode())\n",
    "        else:\n",
    "            if df[column].dtype == int:\n",
    "                df[column] = df[column].fillna(random.randint(df[column].min, df[column].max))\n",
    "            elif df[column].dtype == float:\n",
    "                df[column] = df[column].fillna(random.uniform(df[column].min, df[column].max))\n",
    "            else:\n",
    "                df[column] = df[column].fillna(random.choice(df[column].unique()))\n",
    "\n",
    "# Don't need a function to drop duplicates \n",
    "# df.drop_duplicates()\n",
    "\n",
    "# Give summary of changes made and profiling_summary. Ask user if they are okay with this. If yes, add new cleaned dataframe to cleaned dataframe dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS100Program1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
